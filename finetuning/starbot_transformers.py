# -*- coding: utf-8 -*-
"""Starbot-Transformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16lSuWMVLeIxCabtUdo6EhJCbk_FVynSO

# Starbot Transformers
Final project for CSE 498 G1.

## Importing HuggingFace Transformers
Follows tutorial for Language modeling from [HuggingFace Transformers](https://huggingface.co/docs/transformers/main/en/tasks/language_modeling#masked-language-modeling)
"""

# !pip install transformers datasets evaluate
# from huggingface_hub import notebook_login

# notebook_login()

"""## Import Dataset for Fine-Tuning"""

from datasets import load_dataset
# from google.colab import drive
# import os

# # Load dataset from Google Drive
# drive.mount("/gdrive")
# !ls /gdrive
# BASE_PATH = "/gdrive/My Drive/colab_files/starbot-transformers/"
BASE_PATH = "./"

# Load dataset from CSV
dataset = load_dataset("csv", data_files=BASE_PATH + "general.csv", split="train[:]")
# filter empty
dataset = dataset.filter(lambda e: e["content"] is not None)
# filter links
dataset = dataset.filter(lambda e: "http" not in e["content"])
dataset = dataset.train_test_split(test_size=0.2)
print(f'{len(dataset["train"])} training examples, {len(dataset["test"])} test examples')
dataset["train"]["content"][:10]

"""# Data Preprocessing
## Import Tokenizer
"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

"""## Tokenize Input"""

tokenizer('<startmsg>')

msg_start_token = '<msg>'
msg_end_token = '</msg>'

def preprocess_function(example):
    content = example["content"].lower()
    content = f'{msg_start_token}{content}{msg_end_token}'
    return tokenizer(content)
    
tokenized_dataset = dataset.map(
    preprocess_function,
    remove_columns=dataset["train"].column_names,
)

sample_id = tokenized_dataset["train"]["input_ids"][0]
print(sample_id)
print(tokenizer.decode(sample_id))

"""## Block Input"""

block_size = 128

def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    total_length = (total_length // block_size) * block_size
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_dataset = tokenized_dataset.map(
    group_texts, 
    batched=True,
    num_proc=4
)

tokenizer.decode(lm_dataset["train"]["input_ids"][0])

"""# Padding for Causal Language Modeling"""

from transformers import DataCollatorForLanguageModeling

# tokenizer.bos_token = bos_token
# tokenizer.eos_token = eos_token
tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

"""## Import Model
Model can be changed.
"""

from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

model_id = "distilgpt2"
model = AutoModelForCausalLM.from_pretrained(model_id)

"""## Define Fine-tuning Training Parameters"""

training_args = TrainingArguments(
    output_dir="starbot-transformers",
    evaluation_strategy="epoch",
    num_train_epochs=10,
    learning_rate=0.005,
    weight_decay=0.01,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
)

"""# Train Model"""

trainer.train()
trainer.push_to_hub()

"""# Evaluate Model"""

import math

eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")

"""# Inference"""

from transformers import pipeline
from transformers import AutoTokenizer

# tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
generator = pipeline(
  "text-generation", 
  model="starbot-transformers", 
  tokenizer=tokenizer
)

prompt=f'{msg_start_token}'
inputs = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")
outputs = model.generate(
    inputs, 
    max_length=100,
    pad_token_id=tokenizer.eos_token_id, 
    # eos_token_id=tokenizer(msg_end_token)['input_ids'], 
    # bos_token_id=tokenizer.bos_token_id,
    # do_sample=True,
    temperature=0.7
)
tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].split(msg_end_token)[0][5:]